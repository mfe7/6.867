\section{Convolutional Neural Network (CNN)} \label{sec:prob2}
In this section, we consider the Convolutional Neural Network (CNN) to perform artist identification on paintings.

\subsection{Part 1}
In a convolutional filter, if the first layer applies a 5x5 patch to the image to generate feature $Z_1$, and the second layer applies a 3x3 patch to feature $Z_1$ to generate feature $Z_2$, the receptive field of $Z_2$ (or dimensions of image that affect the node) is 7x7.
That is, a window of 49 neighboring pixels in the original image affects a single node at the output of the filter.
This allows the network to learn spatial features from the original image.
If the conv net becomes deeper (more layers), the network can use larger and more complex combinations of features/regions of the image.

\subsection{Part 2}
We are provided with a conv net (conv.py).
In total, there are [todo] layers: 2 convolutional layers, 1 flatten layer, and 2 dense layers.
The output is the maximum logit of the final dense layer.
[todo] confirm activation function. The hidden layer activation function is relu.
The loss function is softmax cross entropy with logits.
Loss is minimized with Gradient Descent (with a tunable learning rate parameter).

The provided network took about 45 seconds to train on a Macbook CPU.
After 1500 steps, the training accuracy is 87.4\%, and the validation accuracy is 57.5\%.
These numbers suggest overfitting, because the model does not generalize to unseen data very well.

\subsection{Part 3}
Next, we try two common techniques to improve CNN performance.
The first is early stopping.
Early stopping is when training is stopped after validation accuracy levels out, before it starts to decrease, to avoid overfitting.
In this scenario, \cref{fig:2_3_num_steps} shows that early stopping does not make much difference, because the validation accuracy levels off but doesn't decline.
It does have the benefit of shorter training time, though.

\begin{figure}
	\centering
	\includegraphics [trim=0 0 0 0, clip, angle=0, width=0.8\columnwidth,
	keepaspectratio]{figures/2_3_num_steps}
	\caption{Training and validation accuracy are plotted over 10,000 training steps. Model performance levels off after 1,000 training steps.} 
	\label{fig:2_3_num_steps} 
\end{figure}

[todo] pooling layers performance seems to make no difference.

\subsection{Part 4}
Finally, we use the network on a transformed version of the dataset.
Results in \cref{table_2_4} show that the performance is not the same for every transormation type.
For example, the original CNN only gets 10\% accuracy on inverted images, compared to 66.7\% on low contrast images (all relative to a 70.1\% accuracy on normal images).

\begin{table}[ht!]
\centering
\begin{tabular}{||c c||}  
 \hline
 Transformation & Validation Acc (\%) \\ [0.3ex] 
 \hline\hline
 Normal & 70.1 \\ \hline
 Translated & 29.9 \\ \hline
 Brightened & 47.1 \\ \hline
 Darkened & 49.4 \\ \hline
 High Contrast & 63.2 \\ \hline
 Low Contrast & 66.7 \\ \hline
 Flipped & 41.4 \\ \hline
 Inverted & 10.3 \\ \hline
\end{tabular}
\caption{Accuracy of CNNs on transformed dataset.}
\label{table_2_4}
\end{table}



