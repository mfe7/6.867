%!TEX root = main.tex
\section{Neural Networks} \label{sec:prob1}
In this problem, we implement a simple neural network in Python.
The network is trained with backpropogation and stochastic gradient descent.

\subsection{Part 1}
In this problem, we are mainly interested in classification tasks.
Thus, we pass the output of the final hidden layer through a softmax layer to generate a probability (outputs are positive and sum to 1) vector where each element $k$ is the probability of the input being in class $k$.

In general, the training objective is to minimize loss, so at each training step, we evaluate the loss function and compute its gradient, and then adjust the weights and biases in a direction to decrease loss according to the learning rate.
According to the backpropogation algorithm, the derivative of loss with respect to final activation is $\delta^L = Diag[f'(z)] \nabla_a loss$.
For this network with softmax output activation and cross-entropy loss function, we can compute $\delta^L = p(x) - y$, where p(x) is the predicted output for a particular training data point, $x$ in class $y$ (where p is also a function of the network weights, and other parameters).


\subsection{Part 2}
According to the Xavier Initialization, we can initialize weights to be a zero-mean Gaussian with variance [todo].
We initialize biases to zero, but if we initialize weights to zero as well, [todo].

\subsection{Part 3}
To add weight regularization, the objective function would look like:
\begin{equation}
J(w) = l(w) + \lambda(||w^{(1)}||^2_F + ||w^{(2)}||^2_F)
\end{equation}
The only change to the pseudo-code from the lecture notes would be an updated gradient term during the output layer's backpropogation step.

The effect of regularization on the network would be the same as regularization in general.
The $\lambda$ term penalizes weight matrix size, so increasing $\lambda$ causes weight matrices to shrink in size (lower model complexity), which decreases training accuracy (less likely to overfit), but should increase test accuracy, up to a point where $\lambda$ is large enough that the weight term dominates the loss term in the objective function.


\subsection{Part 4}
Next, we use the network for binary classification on the 2D datasets from the previous homework assignment.

\begin{table}[ht!]
\centering
\begin{tabular}{||c c||}  
 \hline
 Dataset & Train & Test (\%) \\ [0.3ex] 
 \hline\hline
 1 & 70.1 \\ \hline
 2 & 29.9 \\ \hline
\end{tabular}
\caption{Accuracy of NN on 2D datasets.}
\label{table_1_4}
\end{table}

\subsection{Part 5}
Finally, we use the network for digit classification on the MNIST dataset.




