%!TEX root = main.tex
\section{Neural Networks} \label{sec:prob1}
In this problem, we implement a simple neural network in Python.
The network is trained with backpropogation and stochastic gradient descent.

\subsection{Part 1}
In this problem, we are mainly interested in classification tasks.
Thus, we pass the output of the final hidden layer through a softmax layer to generate a probability (outputs are positive and sum to 1) vector where each element $k$ is the probability of the input being in class $k$.
While ReLU has a simple activation function and derivative ($f(z) = max(0, z)$ and $f'(z) = 1[x > 0]$), the softmax activation and derivative are slightly more complicated ($f(z)_i = \frac{e^{z_i}}{\sum{e^{z_j}}}$ and $f'(z)_i = [todo]$).