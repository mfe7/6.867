%TODO Figure 4: trajectories are not continuous because of snipping them. That's why a red traj is red and outside the collision area.

\section{Recurrent Neural Network: Prediction} \label{sec:rnn_pred}

\subsection{Introduction}
Previous work has been conducted in predicting trajectories in a 2D space. The most relevant work has used LSTMs to generate human handwriting from the IAM online handwriting database (graves).

LSTMs have the possibility to predict sequences, by feeding themselves their output as an input. To update from binary classification to trajectory predicting LSTM, we change the network architecture from many-to-one to many-to-many. Our output $y_t \Re^{Tx2}$ is computed from every time step as given in ~\cref{eq:eq:bin_class_out}. 

Many work is found on predicting text (hinton,2011). However, our application domain is real-valued in comparison to the discrete valued domain of predicted words.

The literature proposes several transformations, which could be applied instead to the output $y_t$. For example, (Graves) uses mixture density networks (MDNs) and calculates $x_{t+1}$ by the conditional probability $P(x_{t+1}|y_t, )$ and using a Gaussian mixture prior on the probability distribution. The loss is the negative-log likelihood of the conditional probability and SGD is applied to update the weights along with the parameters of the distribution: mean $\mu_m$, standard deviation $\sigma_m$, correlation $\rho_m$, mixture weights $\pi_m$ for $M$ mixture components. However, the complexity of the proposed transition function is obvious and results in longer training time. Due to our limited CPU capacity and corresponding time-consumption of hyperparameter tuning, this approach does not seem promising. 

Additionally, (graves) has used the relative position $\delta x = x_{t+1} - x_t$ as input vector. In our case, we cannot use the relative position, because we would lose the information of proximity to the car. In other words, we would ignore, that a pedestrian might avoid a car, when it approaches him. For the MDN approach our means $\mu_m$ would be further spread among the full absolute position space, which would presumably consume even longer training time with graves approach. Therefore, we have decided to linearly compute the output, as stated in ~\cref{eq:bin_class_out}. 

As we cannot use the loss function proposed by (graves), we have used our loss function, which is the mean squared error over the full output vector and the next ground truth input vector. We have used the $L_2$ norm instead of the $L_1$ norm to be more robust against outliers.

\begin{equation}
Loss_{pred}(x_t) = \frac{1}{T} \sum_{t=0}^{T}(y_t - x_{t+1})^2
\label{eq:loss_pred} 
\end{equation}
%TODO write about last point.

% random batch
The loss function is computed for every batch and updates the parameters in the activation cells via SGD. We randomly draw batches from our training dataset for each SGD parameter update. Thereby we brake apart the dependency of previously concatenated trajectories, that we have split apart into snippets. This is necessary, as a test trajectory will be independent of the training trajectories.

% prediction, parameter-sharing
For prediction, the parameter-sharing property of RNNs and LSTMs is essential. The activation units' parameters have been trained to predict the next state based on the current state. Therefore, for prediction, where the ground truth input vector is unkown, the LSTM can use the output $y_{t-1}$ as input for $x_t$. This gives us the predicted pedestrian position $y_t$.

\subsection{Generated Data}
As seen in the previous sections, the given pedestrian trajectories are highly complex nonlinear functions. To validate our LSTM prediction model, we have trained on the prediction of simpler functions. Therefore, we have chosen to generate linear and sinusodial shaped trajectories, displayed in ~\cref{fig:lin_sin_traj}.

\begin{figure}
	\centering
	\includegraphics [trim=0 0 0 0, clip, angle=0, width=1.0\columnwidth,
	keepaspectratio]{figures/lin_sin_traj}
	\caption{Randomly generated trajectory data in linear (left) shape and sinusodial shape (right). The trajectories have been generated by sampling the start point, range, scale, translation and rotation at random.} 
	\label{fig:lin_sin_traj} 
\end{figure}
%TODO fit linear data
For our hyperparameter tuning, we have considered the batch size $b$, number of training samples $N$, snippet length $T$, learning rate $lr$, maximum epochs $e_{max}$ and number of hidden units $n_h$. 
As the possible combinations rise exponentially with the number of parameters and our computational resources are very limited, we have evaluated most of the parameters, while holding the other parameters constant. Optimal tuning would run a search over all combinations, which would result in better performance.

Each model is evaluated by the training and validation loss, whereas both datasets are generated independently.

In the search over the number of hidden units we have made the following insights. As seen in ~\cref{fig:rnn_hidden}, the number of hidden units $n_h = 1$ is obviously not sufficient to represent the complexity of sinusodial curves. We can also see, that the complexity of $n_h=4$ is sufficient to fit the sinusodial shape approximately and $n_h=32$ is sufficient to visually exactly fit the data. 

\begin{figure}
	\centering
	\includegraphics [trim=0 0 0 0, clip, angle=0, width=1.0\columnwidth,
	keepaspectratio]{figures/rnn_hidden}
	\caption{Search over the number of hidden units: $n_h=1$(top-left), $n_h=4$(top-right), $n_h=32$(bottom-left), $n_h=128$(bottom-right). The plot displays the input trajectory (green) $x_t$ with total trajectory snippet length $T=50$ an its starting point (green-dot). The LSTM predicts the next pedestrian position $y_t$ (blue-dotted) with its starting point (blue-dot), based on the past sequence $x_{0:t}$ and the learned parameters. After the input trajectory (green) is finished, the LSTM tries to predict the future trajectory (red) for $T_p=10$ timesteps. All trajectories are test trajectories and have not been included in the training set.}
	\label{fig:rnn_hidden}
\end{figure}

In ~\cref{tab:rnn_hidden} we can see, that increasing the number of hidden nodes $n_h$ shrinks the loss. The training time was exponently increasing with the number of hidden units, which makes us prefer a lower number of hidden units. A high number of hidden units suggests an overfitting on the training data. However, we can see that our independently generated test data shows similar error as the training set. The complexer the model, the less epochs it took to reach acceptable loss. We further refer to the models with the hyperparameters from ~\cref{tab:rnn_hidden} as 1, 4, 32 and 128-unit model.

We have used early stopping, given a validation dataset, to prevent overfitting. The training has been stopped, when the difference in average batch loss between to epochs $\delta L_b < 0.001$.

\begin{table}[]
\centering
\begin{tabular}{ c| c| c| c| c}
& $n_h=1$ & $n_h=4$ &$n_h=32$ & $n_h=128$\\
\hline
$L_e=0$ & 58.5 & 49.7  & \textbf{6.22} & 2.62      \\
$L_e=1$ & 54.2 & 36.0  & \textbf{0.50} & 0.12      \\
$L_e=2$ & 51.1 & 24.4  & \textbf{0.23} & 0.05      \\
$L_e=3$ & 49.2 & 17.8  & \textbf{0.14} & 0.03      \\
\hline
$e_c$ 			& 25 	& 85 	& 35	& 10 		\\
$L_{e_c}$		&  0.30	& 0.03	& 0.0058& 0.004 	\\
$L_{test, e_c}$	& 0.34	& 0.04	&0.003	& 0.003 	\\     
\end{tabular}
\caption{Search over the number of hidden units $n_h$. The average batch loss ($b=100$) over epochs $e$ and test loss is displayed. Number of training samples $N = 5000$ with snippet length $T=50$ and learning rate $lr=0.01$.}
\label{tab:rnn_hidden}
\end{table}

Include oscillating plot for convergence property.
~\cref{fig:rnn_hidden} also illustrates how LSTM starts by guessing $y_0$ purely based on the input $x_0$. 

% minimum snippet length.
Our tuning showed, that the model needs a minimum snippet length to be able to estimate the curve. For very low snippet length ($T<10$) our loss was increasing ($Loss_b=0.02$ for model $n_h=4$ from ~\cref{tab:rnn_hidden}). An increase in the snippet length over the threshold snippet length $T=50$ was not observed. Therefore, we have set the default snippet length to $T=50$. If we would use RNNs instead of LSTM, the threshold snippet length would be lower, because RNNs have a shorter memory in comparison to LSTMs.

% learning rate, TODO include plot of oscillating learning rate.
The learning rate $lr$ influences the rate of convergence. The higher, the faster the model should converge. However, too high values of the learning rate can cause an oscillation over the minimum. In our case, all learning rates in the interval $[0.1, 0.01, 0.001, 0.0001]$ have been able to find a reasonably low loss. $Lr=0.1$ made the loss oscillate in the interval $[0.01, 0.07]$ for our 32-unit model. A learning rate of $lr=0.01$ for the 32-unit model showed the best tradeoff in terms of minimal loss, while maintaining fast training.

% batch size 
Our choice of batch size $b$ impacted the speed of converges, because tf is possible to execute efficient matrix operations, but did not impact the achievable loss given a fixed model and infinite possible epochs. Given a dataset of the size $N=5000$, we have chosen the batch size to be $b=100$.

%Prediction accuracy.
The LSTM has been designed to predict an unseen pedestrian trajectory for a given number of time steps. Previous work has shown that an LSTM is generally possible to predict a sinusoidial curve into the future (sunsided). However, the found work uses $x_1$ as the input and predicts the $x_2$ value. This is not directly possible in our case, as we have to predict in a local frame with respect to the car. Therefore, we predict both variables at the same time $x = (x_1, x_2)$. Presumably this makes the learning tasks more complex. Additionally, previous work uses more complex models ($n_h=150$) with more training data ($N>500000$) and lower learning rate ($lr<0.0005$). The number of epochs is not stated. An iteration over other hyperparameter choices with these values is not feasible for our computational power and time constraints. Additionally, our domain is more complex. However, the work generally shows that further optimization of the hyperparameters would make it possible to predict trajectories. The red line in ~\cref{fig:rnn_hidden} displays the predictions by our model. 

We conclude that our model is able to predict one step into the future very accurately, given our achieved average batch loss $L_b<0.006$. However, an accurate prediction over many timesteps requires more computational power for hyperparameter tuning. 

\subsection{Real pedestrian data}

We have shown that our LSTM is able continuously predict the next timestep in a domain of translated, rotated, scaled and stretched sinusoidial trajectories. The real pedestrian trajectories are highly nonlinear. LSTMs, in fact neural networks in general, show the capability of fitting a function onto the highly nonlinear data. In ~\cref{fig:ped_data} we can see the trajectories being predicted by our LSTM model.

%TODO add dataset length
We have trained ($N~=3000$), validated ($N~=3000$) and tested ($N~=2000$) on distinct datasets. All hyperparameters have been initially chosen based on the results of the sinusodial trajectories and re-evaluated for the new domain. ~\cref{fig:rnn_real_ped} and ~\cref{tab:rnn_real_ped} shows the performance of our LSTM on real pedestrian data. 
 
The lowest error in the prediction of the consecutive state has been achieved with the highest complexity model with 512 hidden units. This confirms our assumption, that the real trajectories require more complex models to fit the data. The learning rate is chosen $lr=0.001$ and the batch-size is $b=100$. 

\begin{figure}
	\centering
	\includegraphics [trim=0 0 0 0, clip, angle=0, width=1.0\columnwidth,
	keepaspectratio]{figures/rnn_real_ped}
	\caption{Search over the number of hidden units: $n_h=4$(top-left), $n_h=32$(top-right), $n_h=128$(bottom-left), $n_h=512$(bottom-right). The model with the highest complexity is able fit the pedestrian trajectories the best.}
	\label{fig:rnn_real_ped}
\end{figure}

\begin{table}[]
\centering
\begin{tabular}{ c| c| c| c| c}
& $n_h=4$ & $n_h=32$ &$n_h=128$ & $n_h=512$\\
\hline
$L_{e_c}$		&  50.3	& 21.8	& 4.6 & 1.8 	\\
$L_{test, e_c}$	& 18.46	& 4.43	& 0.81	& 0.11 	\\     
\end{tabular}
\caption{Search over the number of hidden units $n_h$ results in the best performance with a model with 512 units. $L_{e_c}$ and $L_{test, e_c}$ is the average batch loss after convergence on the training and test data, respectively.}
\end{table}

The model is able to predict the next pedestrian timestep with an error of $\sqrt{0.11}m$ on the test dataset. As expected, our model was not able to predict pedestrian movement further than several timesteps. We expect better performance with more data and more extensive hyperparameter search. However, pedestrians behave irrational, do not follow predestined rules and different pedestrians behave differently. 